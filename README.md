# Ollama Server

Standalone Ollama instance configuration for EC2.

## Quick Start

1. **Start Server**
   ```bash
   docker-compose up -d
   ```

2. **Pull Model**
   ```bash
   docker exec -it ollama_standalone ollama pull llama3.1
   ```

3. **Test Locally (EC2)**
   ```bash
   curl http://localhost:11434/api/tags
   ```

4. **Test Remotely (Postman)**
   - **Method:** `POST`
   - **URL:** `http://13.233.233.19:11434/api/generate`
   - **Body (JSON):**
     ```json
     {
       "model": "llama3.1",
       "prompt": "Tell me a joke.",
       "stream": false
     }
     ```

   > **Note on Streaming:**
   > *   `"stream": false`: Returns the **whole answer at once** as a single JSON object. Best for Postman testing and simple scripts.
   > *   `"stream": true`: Returns the answer **word-by-word** (like ChatGPT). Best for real-time apps but harder to read in Postman.

     }
     ```

## Understanding the Response JSON

When you get a response, here is what the technical fields mean:

| Field | Meaning | Description |
|-------|---------|-------------|
| **`response`** | **The Answer** | The actual text generated by the AI. |
| **`context`** | **History (Token IDs)** | (Optional) List of tokens for chat memory. Ignore this for independent (stateless) requests. |
| **`prompt_eval_count`** | **Input Tokens** | How many tokens were in your prompt (Question size). |
| **`eval_count`** | **Output Tokens** | How many tokens the AI generated (Answer size). |
| **`total_duration`** | **Total Time** | Total time taken in nanoseconds. (Divide by 1,000,000,000 to get seconds). |

## Server Logs (Debug Mode)

Debug logging is enabled in `docker-compose.yml`. To view detailed processing logs on EC2:

```bash
docker logs -f ollama_standalone
```


To use the `client_tester.py` effectively, pull the supported models on your EC2 server:

```bash
# 1. Main Model
docker exec -it ollama_standalone ollama pull llama3.1

# 2. Alternative Model (Mistral)
docker exec -it ollama_standalone ollama pull mistral

# 3. DeepSeek R1 1.5B
docker exec -it ollama_standalone ollama pull deepseek-r1:1.5b
```

## Testing with Python Client

You can run `client_tester.py` from your local machine (if you clone this repo locally) or from anywhere.

1. **Install dependencies:**
   ```bash
   pip install requests
   ```

2. **Run Tester:**
   ```bash
   python client_tester.py
   ```

3. **Follow Prompts:**
   - Enter your EC2 Public URL (e.g., `http://13.23.45.67:11434`).
   - Choose Model (Llama 3.1 or Mistral).
   - Chat!

